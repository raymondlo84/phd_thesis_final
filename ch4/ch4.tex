\chapter{Three Dimensional High Dynamic Range for 3D Range-Sensing Cameras}
\label{3dhdr3dsensing}

%\section{Abstract}
This chapter presents the invention and implementation of Three Dimensional High Dynamic Range (3DHDR) sensing, along with examples~\cite{lo2013three}. A method for 3DHDR veillance (sensing, computer vision, and video capture) by integrating tonal and spatial information obtained from multiple HDR exposures was proposed for the use in conjunction with one or more 3D (range sensing) cameras.

A 3DHDR camera setup from multiple 3D cameras such as the Microsoft Kinect system was constructed, where the 3D cameras were arranged in a fixed rig such that the calibration parameters between the cameras remain constant over time. With the proposed setup, only a single automated camera calibration step is required at the initial time of assembling and fixing the cameras into the array.  Preferably, the cameras either view from the same position through beam splitters, or are fixed close to one another, so that they capture approximately the same subject matter. The system is designed so the cameras each capture a differently exposed image or video of approximately the same subject matter.  In one embodiment, two Kinect cameras are attached together facing in the same direction, with a neutral density (ND) filter over one of them to obtain a darker exposure.  The dark and light exposures are combined to obtain more accurate 3D sensing in high contrast scenes.  In another embodiment, a single 3D camera is exposure-sequenced (alternating light and dark exposures) to further simplify the setup.

3DHDR might, more generally, be incorporated into existing 3D cameras, resulting in a new kind of 3D sensor that can work in nearly any environment, including high contrast scenes such as outdoor scenes, or scenes where a variable range is needed.

\section{High Dynamic Range Imaging with Range Sensing Camera}

HDR photography~\cite{candocia1, candocia2, candocia3, wyckoff1962experimental, mannist} and video~\cite{mann2012hdrchitecture,lo2012high, mann2012realtime, HDRVideoCamera11, kang2003high} are attractive topics as they address one of the most fundamental problems of digital camera sensors namely their limited dynamic range. Moreover, HDR is not limited to being only applicable to color camera, but it can also be applied to cameras that operate outside the visible spectrum, such as infrared cameras, as well as to 3D sensing cameras such as time-of-flight or structured light camera by combining various exposures in different ranges. 

The light capturing mechanism of cameras, whether it be analog (film) or digital (CMOS/CCD sensors), can only record a limited range of information from any scene at a given time. Combining differently exposed images of the same scene can produce an image of higher range than any of the individual images used to create it. The invention of the active infrared-based 3D sensing camera proposed in this chapter brings forth the added spatial dimension and extends the spectral range that can be captured. Using both a visual light camera and an IR-3D camera allows the capture of an additional spatial dimension and extended spectral range. Applying HDR photography techniques to such a combined system not only increases the dynamic range of the visual camera but possibly also the range of the additional spatial dimension (e.g. making 3D cameras useful outdoors in bright sunlight or in high contrast scenes where glare from bare light bulbs might otherwise degrade the 3D data, or improving the range of the sensor in both long and short distance range).

%Simultaneously on colour and infrared-3D (IR-3D) camera, on a Microsoft Kinect, and thus increases the visual range from the colour cameras and the depth range of the IR-3D camera. 

\subsection{Conventional uses of Depth Sensor: Motion Sensing and Surveillance}

The Kinect system from Microsoft was designed to capture motion with the Microsoft Xbox 360 gaming console.  The Kinect motion sensing system allows the gamer to interact with games without the need for physical controllers. It accomplishes this by tracking the user's movements and position in 3-D space, with respect to itself, in real-time based on random decision forest \cite{criminisi2011decision}. In normal use, the Kinect system is stationary and observes the gamer as he/she moves.  In this sense, the Kinect system was envisioned and created as a surveillance camera (i.e., to be borne by inanimate objects rather than to be worn). 

%\begin{figure*}[!t]
%\centering
%\subfloat[Top]{\includegraphics[height=1.4in]{ch4/diagrams/hdrglass/lowres/IMG_0257.jpg}
%\label{fig_third_case}}
%\subfloat[Front]{\includegraphics[height=1.4in]{ch4/diagrams/hdrglass/lowres/IMG_0263.jpg}
%\label{fig_second_case}}
%\subfloat[Side]{\includegraphics[height=1.4in]{ch4/diagrams/hdrglass/lowres/IMG_0258.jpg}
%\label{fig_first_case}}
%\caption{Wearable Digital Eye Glass Prototype. Our custom 3D printed design allows users to wear the digital glass in everyday life. The proposed algorithm can be utilized for improving the dynamic range of the camera and thus allowing the users to see `better' in daily life. Notice that our proposed system not only allows users to see in high contract scene, it also enables night vision due to the use of active depth sensor which projects IR light patterns and reconstruct 3D information about the scene. These provides users the ability to see in extreme environments, and also allows for robust tracking algorithm to be integrated with the eyeglasses.}
%\label{fig_wearable_glass}
%\end{figure*}

\begin{figure*}
\centering
\begin{subfigure}{.32\textwidth}
\centering
\includegraphics[height=1.2in]{ch4/diagrams/hdrglass/lowres/IMG_0257.jpg}
\caption{Top View}
\label{fig_third_case}
\end{subfigure}
~
\begin{subfigure}{.32\textwidth}
\centering
\includegraphics[height=1.2in]{ch4/diagrams/hdrglass/lowres/IMG_0263.jpg}
\caption{Front View}
\label{fig_second_case}
\end{subfigure}
~
\begin{subfigure}{.32\textwidth}
\centering
\includegraphics[height=1.2in]{ch4/diagrams/hdrglass/lowres/IMG_0258.jpg}
\caption{Side View}
\label{fig_first_case}
\end{subfigure}

\caption{Wearable Digital Eye Glass Prototype. The custom 3D printed design allows users to wear the digital glass in everyday life. The proposed algorithm can be utilized for improving the dynamic range of the camera and thus allowing the users to see ``better''. Notice that the proposed system not only allows users to see in high contrast scenes, it also enables night vision with the use of active depth sensors which project IR light patterns and reconstruct 3D information about the scene. This provides users the ability to see in extreme environment, and also allows for more robust tracking algorithms to be integrated with the eyeglasses~\cite{lo2013three}.}
\label{fig_wearable_glass}
\end{figure*}


\subsection{Reversing the role of the user and camera: Sousveillance}
The use of the Kinect 3D sensing cameras in a different manner is proposed in this chapter, where the Kinect camera moves with the user so that it observes the real world in a similar fashion as the user observes it (or would have observed, in the case of a blind individual). Rather than having the Kinect camera watch the user, the user uses it to watch the environment. In one of the wearable implementations as shown in Fig.~\ref{fig_wearable_glass}, the Kinect camera is used to extract depth information from the environment being observed by the user.

There are numerous potential applications for such a system:
\begin{itemize}
\item Seeing and sensing aid~\cite{mann2011blind}
\item EyeTap wearable Digital Eye Glass~\cite{lo2012high, mann2012hdrchitecture, mann2012realtime}
\item 3D (sur/sou)veillance 
\item 3D scanning and reconstruction~\cite{izadi2011kinectfusion}
\item Surface tracking and gesture input for wearable computers requiring both long and short range scanning of the environment~\cite{newcombe2011kinectfusion}
\end{itemize}

\subsection{The fundamental problem of sousveillance: Dynamic Range}
\begin{figure*}
\centering
\includegraphics[width=3.0in]{ch4/diagrams/high_expo_light.jpg} 
\includegraphics[width=3.0in]{ch4/diagrams/no_light_depth.jpg} \\
\caption{Fundamental dynamic range issues in depth-sensing cameras. The limited dynamic range of the sensor often creates blind spots in the scene. In this case, the light bulb blinded the sensor completely leaving a white patch on the scene even with an infrared camera. To overcome this, we can also deploy HDR approaches described in previous chapters~\cite{lo2012high,lo2013three}.}
\label{fig:ir_range_issue}
\end{figure*}

Generally, surveillance is the monitoring of one's own property such as one's own living room, i.e., a space that one has control over. With surveillance, we can carefully place the camera in the optimal location. For example, in using the Kinect camera to observe a typical living room, the user will often give careful consideration to its placement, being sure to avoid placing it where sunlight might shine in from windows. The user may move lamps out of its field of view, in order to reduce glare that prevents it from working properly.  On the other hand, sousveillance involves seeing in a property that might not belong to the wearer, and the assumption of being static is often invalidated. For example, the gesture-sensing camera or seeing aid must function in any environment where we cannot demand the owner of the space to remove lamps that may be causing glare or similar problems.

When using the Kinect camera as a seeing aid, for self gesturing (gesture-sensing wearable applications), and marker or markerless surface tracking, it would be unreasonable to demand the environment to conform to the user's needs. For example, Fig.~\ref{fig:ir_range_issue} is a common scene where a light bulb is presented in an indoor environment. When the light bulb is turned on, the sensor fails to pick up any information around the bright area due to the limited dynamic range of the sensor. 

Thus, the problem of dynamic range must be solved in the sousveillance device itself without requiring a  change in the environment. Fortunately, advances in sensor technology have enabled the miniaturization of sensors and their use in a robust camera array setup. One way to address the dynamic range limitation is to combine various sensor signals and create a superset which surpasses the capability of any individual sensor alone.

\subsection{Technology Behind the Microsoft Kinect 3D Sensing System}
The Microsoft Kinect system employs the PrimeSense 3D sensing technology. The PrimeSense 3D sensor uses light coding to compute the scene volume, using active infrared (IR) illumination~\cite{shpunt2008depth,shpunt2010optical,shpunt2007depth}. The sensor then uses a CMOS image sensor to read the light coding infrared patterns back from the scene as shown in Fig.~\ref{fig:ir_range_issue}. The coded light is processed by the PrimeSense SoC~\cite{spektor2009integrated}, contained in the 3D sensor, to calculate depth information. In many cases, one may notice that the infrared sensors may fail to observe objects which have high reflectance property (glossy surfaces) or high absorbance property (matte black finish surfaces). In these difficult cases, the idea of HDR comes in as by carefully adjusting the exposures, the sensors can adapt and observe what was not possible to see otherwise.

\begin{figure*}
\centering
\includegraphics[width=3.0in]{ch4/diagrams/low_expo_light.jpg} 
\includegraphics[width=3.0in]{ch4/diagrams/high_expo_light.jpg} \\
\caption{HDR image set with infrared-based depth sensing camera. By adding an ND-filter on top of the camera or alternating the exposure settings, two separate exposures of the same scene can be acquired. By combining both exposures, we can obtain an HDR image which improves not only in tonal range, but also the spatial range as the lower exposure image can now see the over-exposed foreground, while the higher exposure image can now see the darker background~\cite{lo2013three}}
\label{fig:ir_hdr}
\end{figure*}

\section{Background and Theory}
In this section, we describe the geometric representation of the cameras and the formation of images with a pinhole camera model. These provide the fundamental principles for reconstructing images with the depth information from multiple Kinect cameras.  For consistency, the notation used in this chapter closely follows the conventions in~\cite{wei1994implicit,zhang2000flexible}. Then, we extend the discussion into High Dynamic Range (HDR) image composition with 3D data points. Here, we describe our novel algorithm that merges the differently exposed sets from the 3D data acquired with multiple Kinect cameras.

\subsection{Pinhole Camera Model and Perspective Projection}
To understand the geometric relationship between the camera and the scene, we first assume that the camera follows a pinhole camera model. That is, a scene view is formed by projecting a 3D point onto the image plane using a perspective transformation. 

Let's denote a 2D point on an image plane as $\mathbf{m}=[u, v, 1]^{T}$, and a 3D point in the scene as $\mathbf{M}=[X, Y, Z, 1]^{T}$, in homogeneous coordinates.

With a pinhole camera model, the relationship between 3D points $\mathbf{M}$ and its image projection $\mathbf{m}$ is given by
\begin{equation}
s \mathbf{m} = \mathbf{A} [ \mathbf{R} ~ \mathbf{t} ] \mathbf{M}, 
\label{pinhole_model}
\end{equation}
where the extrinsic parameters $[\mathbf{R} ~ \mathbf{t}]$ describe the translation and rotation relationship between the real-world coordinate system and the camera coordinate system. 

The matrix $\mathbf{A}$ in Eq. \ref{pinhole_model} is called the intrinsic parameters of the camera, and is given by 
\begin{equation}
\mathbf{A} = 
\begin{bmatrix} 
  \alpha & \beta & u_{0}\\ 
  0 & \gamma & v_{0} \\
  0 & 0 & 1 \\  
\end{bmatrix}.
\label{intrinsic_parameters}
\end{equation} 
In Eq.~\ref{intrinsic_parameters}, ($u_{0}$, $v_{0}$) is the principal point, and $\alpha$, $\beta$, and $\gamma$ describe the scaling as well as the skew of the image in the $u$ and $v$ axes, respectively. In general, the camera may have a single focal length lens, and these intrinsic parameters only need to be estimated once per setup.
%$s[u v 1] = [fx 0 cx 0 fy cy 0 0 1][r11 r12 r13 r14 t1 ][X Y Z 1]$
\begin{figure}
\centering
\includegraphics[width=2.5in]{ch4/diagrams/multi_virt_cam2.pdf} \\
%Camera Configuration and virtual cameras
\caption{This figure illustrates one of our proposed configurations. Each Kinect camera ($C_1$, $C_2$) provides a 3D measure of the scene. With the 3D data, we can reconstruct the scene and re-position $C_1$ and $C_2$ as a virtual camera $V$ at any arbitrary position. This configuration allows us to perform HDR composition as if the images were simultaneously captured from the same camera. The distance between the cameras $d$ can be estimated using a camera calibration method \cite{lo2013three}.}
\label{fig_v_camera}
\end{figure}
\subsection{Lens Distortion Correction}
Real lenses suffer from various level of distortions. For simplicity, we address the lens distortion problem by considering the radial and tangential distortion correction which can be modelled by a simplified Brown's model~\cite{brown1966decentering}
\begin{equation}
\begin{split}
%& x = (u - u_{0})\alpha,~y = (v-v_{0})\beta, \\ 
%& \bar{x} = x + \delta^{(x)}(x,y),~\bar{y}=y+\delta^{(y)}(x,y),\\
%& \delta^{(x)}(x,y) = x + x[k_{1} (x^2 + y^2) + k_{2} (x^2+y^2)^2], \\
%& \delta^{(y)}(x,y) = y + y[k_{1} (x^2 + y^2) + k_{2} (x^2+y^2)^2] \\
r &= \sqrt{(x^2+y^2)}, \\
x' &= x(1+k_1 r^2+k_2 r^4)+p_1(r^2+2x^2)+2p_2xy, \\
y' &= y(1+k_1 r^2+k_2 r^4)+p_2(r^2+2y^2)+2p_1xy
\label{eq_lens_distortion}
\end{split}
\end{equation}
where $(x', y')$ are the ideal pixel image coordinates after the distortion correction, $(x, y)$ are the real observed image coordinates centered at the principal point, and $k_{1}$, $k_{2}$, $p_1$, $p_2$ are the coefficients of the radial and tangential distortion, respectively.  These parameters can be estimated using a camera calibration method by providing a known geometry to the scene. One example would be to use a checkerboard pattern on a planar surface, and this technique will be discussed in the next section.

\subsection{Camera Calibration}
%Camera calibration is the preliminary step toward computer vision.
In order to combine the tonal and spatial information for our proposed setup, it is necessary to calibrate the cameras (i.e., estimate the extrinsic and intrinsic parameters described previously, and also estimate the camera response function for correcting the non-linearity in the HDR composition process.) Camera calibration is a well-studied problem in the computer vision field and numerous techniques have been proposed \cite{zhang2000flexible, mannist, robertson2003estimation} for correcting and estimating these parameters. Their implementations can be found in the common computer vision libraries such as the OpenCV camera calibration tools\footnote{Camera calibration with OpenCV:~\url{http://docs.opencv.org/2.4/doc/tutorials/calib3d/camera_calibration/camera_calibration.html}}.
\begin{figure}
\centering
\includegraphics[width=3.4in]{ch4/diagrams/kinect_closeup} \\
\caption{Our proposed configuration with two Kinect cameras. The upper Kinect camera is augmented with a polarized neutral density filter for manual adjustment of the exposure.}
\label{fig_multiple_camera}
\end{figure}


%XXX DESCRIBE THE CALIBRATION PROCESS XXX?
%Calibration techniques can be classified into one of the two categories: self-calibration and photogrammetric calibration. One key advantage of photogrammetric calibration technique is that 

\subsubsection{Geometric Calibration}
For our configuration, we have the Kinect cameras positioned on top of each other as shown in Fig.~\ref{fig_v_camera}. It is possible to have these cameras further apart or closer together. However, the advantage of placing the camera closer together is that the cameras share a common field of view, which reduces the `holes' due to occlusion. Since the relative position of the cameras is fixed in our setup, the calibration process only needs to be computed once which increases the flexibility and portability of our approach.

Equations (\ref{pinhole_model}) and (\ref{intrinsic_parameters}) provide a model for the camera relations. Given an image set of some known planar pattern (such as a checkerboard), we can recover the intrinsic and extrinsic parameters of the cameras in (\ref{pinhole_model}) using the methods in \cite{zhang2000flexible}. Additionally, the lens distortion parameters described in (\ref{eq_lens_distortion}) are also modelled and estimated from the process.

Once these parameters are estimated, the 3D data from each camera can be mapped to the real-world coordinates, thereby allowing us to combine the data in the next step. 

%The initial step we first estimate the distortion parameters, intrinsic parameters of the individual cameras. This can be extracted easily by providing a xxx. 

In addition, we estimate the relative pose, $[\mathbf{R_{i}~ t_{i}}]$, for each $i$th Kinect camera in the real-world coordinates to allow for global registration of the 3D data. By setting one of the Kinect cameras as a reference (origin), we can then describe the relative positions of the Kinect cameras using simple matrices. All parameters extracted from our setup are summarized in Table~\ref{tab_calibration}.

\subsubsection{Photometric Calibration}
Another key challenge in calibrating the Kinect camera is its lack of exposure controls. Instead of programming the camera for different exposure settings, in our configuration, a polarized variable neutral density filter was attached to one of the Kinect cameras to reduce the exposure as shown in Fig.~\ref{fig_multiple_camera}. To capture differently exposed images on a Kinect camera, we programmed the Kinect camera for manual exposure, and thus the Kinect camera does not compensate for any changes to the lighting condition and alter the exposures unexpectedly.  Then, we manually adjust the filter to extract a set of images that vary in some known exposures for the recovery of the camera response function\cite{mannist}.

%\begin{figure}
%\centering
%\includegraphics[width=1.5in]{ch4/diagrams/bright.png} 
%\includegraphics[width=1.5in]{ch4/diagrams/dark.png} \\
%\includegraphics[width=1.5in]{ch4/diagrams/bright_depth.png} 
%\includegraphics[width=1.5in]{ch4/diagrams/dark_depth.png}
%%Camera Configuration and virtual cameras
%\caption{Multiple Exposures captured by the Kinects. The slight change in the camera position results in two images of the same scene rendered from different perspectives. The depth map is not affected by the darkening because the structured light code pattern is invariant to small changes in exposures. However, there is an increase in dynamic range in high contrast scene when the IR images were overexposed.}
%\label{fig_v_camera}
%\end{figure}

\begin{figure*}
\centering
\includegraphics[width=6.5in]{ch4/diagrams/hdr_3d_all.pdf} 
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/off_light_filter/depth.png} 
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/off_light_filter/ir.png} 
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/off_light_filter/rgb.png} \\
%(a) Tungsten light off with a variable ND filter attached \\
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/off_light/depth.png} 
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/off_light/ir.png} 
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/off_light/rgb.png}   \\
%% \vspace*{3pt}
%(b) Tungsten light off\\
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/on_light_filter/depth.png} 
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/on_light_filter/ir.png} 
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/on_light_filter/rgb.png} \\
%(c) Tungsten Light on with a variable ND filter attached \\
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/on_light/depth.png} 
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/on_light/ir.png} 
%\includegraphics[width=1.1in]{ch4/diagrams/kinect_ir_color_3/on_light/rgb.png}  \\
%(d) Tungsten light on\\
%Camera Configuration and virtual cameras
\caption{Notice that the depth map is not significantly affected by the darkening filter. The structured light code pattern is invariant to small exposure differences; however, there is a significant increase in dynamic range in high contrast scenes when the IR images were overexposed due to the tungsten light bulb. The depth composition algorithm is described in Section~\ref{sec_hdr_depth_map} \cite{lo2013three}.}

\label{fig_all_camera}
\end{figure*}
\begin{figure*}
\centering
\includegraphics[width=7in]{ch4/diagrams/jason_hdr.pdf} 
\label{fig_jason_dep_hdr}
\caption{Depth map composition using our proposed algorithm. We combine two different depth maps by performing a weighted sum over the certainty function described in Eq.~\ref{eq_depth_sum}. There is a slight contour in the area where the depth map is undefined as shown in the right most image. These can be addressed by applying spatial filters on the depth map based on the geometry~\cite{lo2013three}.}
\end{figure*}
\subsubsection{Summary}
%\begin{figure}
%\end{figure}


%describe the algorithm behind the registration and calibrations behind the camera
%show the rgb images
%alignment algorithm(s): using checker board
%and it is a real-time process? because once it is calibrated, our setup wont' require any other calibrations
\begin{table}[h]\footnotesize
\centering
  \caption{Parameters estimated from the camera calibration process}
  \label{tab_calibration}
    \begin{tabular}{|p{2.0cm}|l|p{4.0cm}|}
        \hline
       \bf{Parameter(s)}        &  \bf{Type}      &  \bf{Description}                                                        \\ \hline
        $\mathbf{A}_{ci}$, $\mathbf{A}_{ri}$, $\mathbf{A}_{cj}$, $\mathbf{A}_{rj}$  & Intrinsic & Focal length, principal points, and skews for the color camera and the infrared camera of the $i$th and $j$th Kinect camera \\ \hline
        $kc_{1i}$, $kc_{2i}$, $pc_{1i}$, $pc_{2i}$, $kc_{1j}$, $kc_{2j}$, $pc_{1j}$, $pc_{2j}$   & Intrinsic & Lens distortion coefficients for the color camera of the $i$th and $j$th Kinect camera \\  \hline
        $kr_{1i}$, $kr_{2i}$, $pr_{1i}$, $pr_{2i}$, $kr_{1j}$, $kr_{2j}$, $pr_{1j}$, $pr_{2j}$  & Intrinsic & Lens distortion coefficients for the infrared camera of the $i$th and $j$th Kinect camera \\ \hline
        $\mathbf{R}_i$, $\mathbf{t}_i$             & Extrinsic & Geometric relations between the infrared camera and the color camera of the $i$th Kinect camera \\ \hline
        $\mathbf{R}_{ij}$, $\mathbf{t}_{ij}$    & Extrinsic & Geometric relations between the $i$th and the $j$th Kinect cameras in the scene \\
        \hline
        $f_r(q)$, $f_g(q)$, $f_b(q)$    & Photometric & The camera response function for each color channel\\
        \hline
    \end{tabular}
\end{table}

%table that summarize what's going to be extracted from the calibration process
Table~\ref{tab_calibration} summarizes the parameters required for the 3DHDR reconstruction step. For our experiment, we have used two Kinect cameras and the first Kinect camera is set as the reference camera. For example, the matrices $\mathbf{R}_{21}$ and $\mathbf{t}_{21}$ define the transformation of how 3D data is transformed from the second Kinect camera to the first Kinect camera. Finally, the photometric calibration procedure provides us with a camera response function that describes the non-linear relationship in both the IR and the color cameras.

\subsection{3D Point Cloud Reconstruction from Kinect Cameras}
Each of the Kinect cameras provides a 3D measure of the scene with the color camera and the infrared-based depth camera as shown in Fig~\ref{fig_multiple_camera}. The IR camera receives the structured light pattern emitted by the IR laser projector for depth reconstruction. The depth measure $[x_d, y_d, z]^T$ corresponds to a point $[X_i,Y_i,Z_i]^T$ in the real-world coordinates, and the conversion from image coordinates (in pixel) to real-world coordinates (in meter) can be computed with the following equations:
\begin{equation}
\begin{split}
X_i &= (x_d - p_x) * z / f_d,\\
Y_i &= (y_d - p_y) * z / f_d, \\
Z_i &= z, \forall \{ z_{min}<z<z_{max}\}
\label{3D_kinect_color}
\end{split}
\end{equation}
where ($x_d$, $y_d$) are the image or pixel coordinates of the depth map image, $z$ is the depth measurement in meters, ($p_x$, $p_y$) is the principal point, $f_d$ is the focal length of the depth camera, and $i$ is the index for the $i$th point in space. In case z is out of the range between $z_{min}$ and $z_{max}$, we simply ignore these values in the computation.

Since the position of the IR and color cameras are fixed, we can relate the depth information to the color image by performing a transformation using the intrinsic and extrinsic parameters estimated previously. To extract the RGB component for each point $\mathbf{M_i} = [X_i, Y_i, Z_i, 1]^T$, $i=1,..,m$ where m is the number of valid depth readings from the Kinect cameras, we project $\mathbf{M_i}$ onto the the camera coordinates of the color camera using the intrinsic and extrinsic parameters estimated earlier to obtain the pixel coordinates in the RGB image, and then we aggregate these values to create our point cloud data point $\mathbf{\tilde{M}_{i}}$ along with its exposure $k_i$ 
\begin{equation}
\begin{split}
[u_i'~v_i'~1]^T &=  \mathbf{A}_{ci}[\mathbf{R}_i~\mathbf{t}_i]\mathbf{M}_i \\
[r_i, g_i, b_i, k_i]^T &= \mathbf{I}(u_i', v_i')
\label{eq_3D_rgb_lookup}
\end{split}
\end{equation}
where $\mathbf{R}_i$ and $\mathbf{t}_i$ define the rotation and translation of the camera position from the IR camera to the color camera coordinates. Since the projection may result in a non-integer pixel coordinate for $(u_i', v_i')$, we would estimate its value by performing a linear interpolation on the neighboring pixels in the color image $\mathbf{I}$. From each Kinect camera, we construct a set of point cloud data points $\mathbf{\tilde{M}_{i}} = [X_i, Y_i, Z_i, r_i, g_i, b_i, k_i]^T$ which consists of a 7-dimensional vector of the spatial and tonal information about the scene.


%\section{3D High Dynamic Range Imaging with a single Kinect} 
%\section{3D High Dynamic Range Imaging with a single Kinect} 
\section{High Dynamic Range Depth Map}
\label{sec_hdr_depth_map}
The Kinect camera contains the source of the projected IR pattern as well as the IR image sensor. Based on the observation of the distortion in the pattern of the scene, the device is able to compute the distance of the objects in the view. The observation, however, is affected by the material property and the distance of the objects. The objects are usually overexposed at close proximity to the camera and the pattern reflected off  objects at far distances are typically too dark and underexposed. This becomes a major limitation in the ability of the camera to accurately determine the distance of objects that are not well exposed. Due to the limited dynamic range per exposure setting, the perceived patterns by the infrared camera are lost in the over- or under-exposed areas of the scene.

The dynamic range problem can be resolved by capturing multiple exposures of the scene using the infrared camera. At low exposure, the details of closer objects are more visible to the IR sensor, while at high exposure, the details of objects farther away become more visible. For depth maps recovered from differently exposed IR images, the unknown depths, due to the lost details from one exposure, can be compensated by another. This collection of depth maps can be used to compute a high dynamic range depth map, which enables more robust depth calculation under challenging lighting conditions.

\subsection{Comparagram of Depth Map}

A comparagram is a cross histogram of two images of different exposures \cite{mannwyckofftr}. A comparagram reveals the comparametric relationship of the outputs of the same camera with different exposure settings. For the purpose of extending the depth map range based on the Wyckoff set of depth maps, we attempt to directly observe the relationship between the depth maps produced by the infrared images from different exposure settings. The resulting comparagram in Fig.~\ref{fig_various} shows that the known depth values from the images of the Wyckoff set are the same. Therefore, the response $f(*)$ of the depth value $d$ is simply $f(d) = d$.

\subsection{Compositing of Depth Map}
The resulting comparagrams in Fig.~\ref{fig_depth_parts} and Fig.~\ref{fig_all_camera} show that the depth values are invariant to slight changes to exposures on the infrared sensor. In the case of a scene with IR light source, a darkening filter can improve the depth map reconstruction because it reduces the exposures (see Fig~\ref{fig_all_camera}) in the over-saturated area. To combine the depth maps for HDR reconstruction with a single Kinect camera, we adopt the composition method described by \cite{mannwyckofftr} and apply certainty (or weight) on the estimated distance values of different depth maps. In contrast to \cite{mannwyckofftr}, the certainty function for the distance value is not based on the camera response of the infrared sensor. Instead, we design the certainty function based on the sensitivity of the depth map to distance conversion. Let's denote $g(r)$ as the distance response function that converts the known distance $r$ of the objects in meters to depth values and $g^{-1}(d)$ as its inverse response. We obtain the certainty function by taking the derivative of $g(r)$ with respect to distance. According to \cite{mann2011blind}, we could convert the disparity map to the depth map using the following equation:
\begin{equation}
r = g^{-1}(d) = 1/(\alpha \cdot d + \beta)
\end{equation} 
where $\alpha$=$-0.0030711016$ and $\beta$=$3.3309495161$ are the parameters extracted from the least squared regression. Its certainty function \begin{equation}
c(r) = \frac{\delta g(r)}{\delta r} = 
\begin{cases} \frac{-1}{\alpha \cdot r^2}, 
			& \mbox{if } r_{min} \le r \le r_{max} \\
			0, & \mbox{otherwise}
 \end{cases}
\end{equation}
where $r_{min}$$\approx$$0.3$ m and $r_{max}$$\approx$$5.3$ m are calibrated based on our Kinect cameras.
Finally, the joint estimate of distance from two sensors with different exposure settings is expressed as the weighted sum per pixel
\begin{equation}
r_{joint} = \frac{\sum c(r_i) \cdot r_i}{\sum c(r_i)}
\label{eq_depth_sum}
\end{equation}
where $r_i$ denotes the estimated distance from the $ith$ sensor.
\begin{figure*}
\centering 
\includegraphics[width=6.5in]{ch4/diagrams/jason_depth.pdf} 
\caption{Image pairs used for comparagram construction. Three different Wyckoff sets of two depth maps were captured to cover the full range of depth values. The exposure difference between the bright and the dark infrared image is 1 EV apart. The resulting depth map comparagram per Wyckoff set is shown in the right most column of the figure. Each covers a partial range of depth values in its comparagram~\cite{lo2013three}.} 
\label{fig_depth_parts} 
\end{figure*}
%\begin{figure} 
%\centering 
%\includegraphics[width=0.5\columnwidth]{ch4/diagrams/jason_comparagram.pdf} 
%\caption{The aggregated comparagram using all comparagrams constructed of partial depth range in Fig.
%\ref{fig_depth_parts}.}
%\label{fig_aggregated} 
%\end{figure}
\begin{figure*} 
\centering \includegraphics[width=6.5in]{ch4/diagrams/jason_collection.pdf} 
\caption{Comparagram of depth maps taken at various exposure differences. The comparagrams exhibit the same linear mapping of depth values obtained from infrared image pairs that are 1/3, 2/3, 1 or 4/3 stop apart~\cite{lo2013three}.}
\label{fig_various} 
\end{figure*}


\section{3D High Dynamic Range Composition with Kinect Cameras} 
%High Dynamic Range imaging has been discussed extensively in the literature. By combining differently exposed images of the same subject matter, we can further extend the dynamic range of the sensors. 

%In Fig.~\ref{fig_all_camera}, we see that it is possible to capture a higher dynamic range with the Kinect by taking multiple exposures of the same subject matter with a Kinect that is augmented with a polarized variable ND filter. 
In this section, we will discuss our novel technique for combining the data for producing an HDR 3D point cloud that has higher dynamic range in both tonal and spatial range that cannot be possibly captured by a single exposure. For simplicity, we will first discuss the approach with a single Kinect for capturing a static scene. The images are captured by adjusting the exposure on the variable ND filter and we assume that the camera and the scene are both static. Then, we propose an approach for constructing 3DHDR from data captured simultaneously from multiple Kinect cameras. One key contribution of this chapter is the novel approach and the unique configuration that enable simultaneous capturing, reconstruction, and rendering of 3DHDR data stream from 3D range sensing cameras. 

\subsection{Background}
In our configuration, we capture a set of images, possibly synchronously among the Kinect cameras, to produce two point cloud data sets that capture differently exposed images of the scene along with their 3D data: $\mathbf{\tilde{M}_{i}}$ and $\mathbf{\tilde{N}_{j}}$, $i=1,..,m$, $j=1,..,n$, where m and n are the number of point cloud data points reconstructed from the Kinect cameras.

We denote $f$ and $f^{-1}$ as the camera response and the inverse camera response functions, respectively. A pixel, $p$, is the camera response of quantigraphic measure, $q$, at a selected exposure level, $k$, which can be expressed as $p = f(kq)$. Inversely, the estimate of $q$ from a single exposure is $\hat{q} = f^{-1}(p)k_i^{-1}$. Thus the joint estimate $\hat{q}_{joint} = f_{joint}(k_1\hat{q_1}, .., k_i\hat{q_i}, .., k_n\hat{q_n})$ can be computed using the methods proposed by \cite{mannist,robertson2003estimation,ali2012ICASSP} based on a Wyckoff set \cite{wyckoff1962experimental} of size $N$, for $N \ge 1$.

\subsection{HDR Composition with a Single Kinect}
\label{sec_single_kinect}
Similar to the approach proposed in \cite{lo2012high}, we can easily combine the differently exposed images using the weighted sum approach. The HDR image undergoes a real-time tonemapping algorithm described in the previous chapter~\cite{lo2012high} to display on LDR displays. Additionally, the HDR composition approach will create a resulting depth map that has fewer missing depth points than any of the single depth map. Using Equations (\ref{3D_kinect_color}) and (\ref{eq_3D_rgb_lookup}), we can then create a 3D point cloud as our desired HDR output. 

\subsection{Simultaneous HDR Composition with Multiple Kinect Cameras}

\begin{figure}
\centering
\includegraphics[width=1.7in]{ch4/diagrams/corners01.jpg} 
\includegraphics[width=1.7in]{ch4/diagrams/corners02.jpg} \\
\includegraphics[width=1.7in]{ch4/diagrams/calibration_camera1.jpg} 
\includegraphics[width=1.7in]{ch4/diagrams/calibration_camera2.jpg} \\
%Camera Configuration and virtual cameras
\caption{Camera calibration and 3D projection of the point cloud data in world coordinates. Notice that with our setup, one camera is upside down and the camera calibration is affected by the orientation and perspective difference. The corners from the checkerboard are used for the estimation of $[\mathbf{R}_{ij}~\mathbf{t}_{ij}]$ in the camera calibration step. The images in the second row are the projection of the point cloud data from two separate Kinect cameras $C_{1}$ and $C_{2}$ onto the virtual camera $V$, and rendered with OpenGL. Note that with our configuration, the Kinect cameras can capture differently exposed images at the same time~\cite{lo2013three}.}
\label{fig_reproject}
\label{sec_sim_hdr_mul_kinect}
\end{figure}

In contrast to the typical HDR composition algorithm where the composition is performed on two images from the same image plane, we propose an algorithm that operates on the 3D data from multiple Kinect cameras that are captured from two different perspectives simultaneously. That is, instead of being concerning with the tonal information only, we consider additional spatial parameters in the HDR composition process.  One key advantage of this approach is that we can simultaneously capture the information from multiple perspectives, thereby reducing the registration problem due to camera, or in-frame, motion. Furthermore, this allows us to capture real-time 3DHDR (30 fps) video of the scene. However, there are several trade offs associated with this approach and these limitations are discussed in Section~\ref{sec_discussion}.

\subsection{3D Image Registration}
The Kinect camera provides us a 3D measure of the scene. We can register the point cloud data $\mathbf{\tilde{M}}_{i}$ and $\mathbf{\tilde{N}}_{j}$
 from the differently exposed RGB images by projecting the cameras $Ci$ to $Cj$ and vice-versa based on their geometric relations estimated in the camera calibration process.

In Equation~\ref{eq_3D_rgb_lookup}, we described the image registration process for obtaining the RGB image from the depth map on a Kinect camera. Similarly, we can extend such a mapping by first transforming the $j$th Kinect camera to the $i$th Kinect camera's coordinates and then obtain the RGB value from the $i$th Kinect device's color camera 
\begin{equation}
\begin{split}
[u_i'~v_i'~1]^T &=  \mathbf{A}_{ci}[\mathbf{R}_i~\mathbf{t}_i][\mathbf{R}_{ji}~\mathbf{t}_{ji}]\mathbf{N}_j\\
[r_i, g_i, b_i, k_i]^T &= \mathbf{I}(u_i', v_i').
\label{eq_3D_rgb_lookup_ir}
\end{split}
\end{equation}

Now, for each $\mathbf{\tilde{M}}_{i}$ and $\mathbf{\tilde{N}}_{i}$ we have found a corresponding pair $[r_i, g_i, b_i, k_i]$ and $[r_j, g_j, b_j, k_j]$ for the HDR composition step.  Following the steps described in Section~\ref{sec_single_kinect}, we can compose an HDR image as if the color images were captured from the same camera as shown in Fig.~\ref{fig_reproject}.

\subsection{3D Point Cloud Merging}
\begin{figure}
\centering
\includegraphics[width=3.3in]{ch4/diagrams/3d_data_structure.pdf} \\
%Camera Configuration and virtual cameras
\caption{Compositing of the point cloud data sets $\mathbf{\tilde{M}}_i$ and $\mathbf{\tilde{N}}_j$ in 3D space. The point cloud data from each Kinect camera are projected onto the real-world coordinates and merged based on the tonal and spatial information in Eq.~\ref{eq_match_3d_point}. $d_{ij}$ is the distance between two point cloud data points in meters and $r$ is the search radius.}
\label{fig_3D_structure}
\end{figure}
The approach discussed in Section~\ref{sec_sim_hdr_mul_kinect} suggests that we could perform 3D composition of the RGB data by re-projecting the cameras onto the same image plane. Instead, we can operate directly on the point cloud data and merge the data directly in 3D space.

Given two point cloud data sets  $\mathbf{\tilde{M}_{i}}$ and $\mathbf{\tilde{N}_{j}}$ where $i$, $j$ are the indices for the $i$th point and the $j$th point in space, we compute their new correspondence sets by finding the candidate with the best score in the similarity metrics 
\begin{equation}
\label{eq_match_3d_point} 
\begin{split}
I(i,j) &= (w_{ij}\mathbf{d}_{ij} + c_{ij}\mathbf{l}_{ij})/(w_{ij} + c{ij}), \\
\mathbf{d}_{ij} &= || \mathbf{X}_i - \mathbf{X}_j ||, \\
\mathbf{l}_{ij} &= ||{k_i\mathbf{Q}(r_i,g_i,b_i)-k_j\mathbf{Q}(r_j,g_j,b_j)}||
\end{split}
\end{equation}
where $\mathbf{X}=[X, Y, Z]$ is the position in the real-world coordinates, $\mathbf{Q}(r,g,b)$ is the light quantity vector $[f^{-1}(r), f^{-1}(g), f^{-1}(b)]^T$ based on the camera response function, $k_i$ and $k_j$ are the exposure values, and $w_{ij}$ and $c_{ij}$ are weight functions on the spatial and tonal data. 
The weighted functions can be defined as a penalty on the uncertainties in the point cloud data 
\begin{equation}
\begin{split}
c_{ij} &= s_c \\
w_{ij} &= s_d/(e(Z_i) + e(Z_j))
\end{split}
\end{equation}
where $c$ and $e$ are the certainty functions of the depth map and the color image, and $s_d$ and $s_c$ are the scaling variables for the weight.

To reduce the complexity of the matching process, a k-d tree data structure is used to the search for potential candidates in close proximity. For data points that have no matches, perhaps due to shadows, or `holes' in the depth map, we simply add those data points to the final result by remapping the $[r, g, b]$ values into light vectors $[q_r, q_g, q_b]$. 

%\begin{figure}
%\centering
%\includegraphics[width=1.5in]{ch4/diagrams/hdr_3d_algorithm.pdf} \\
%%Camera Configuration and virtual cameras
%\caption{The tonal information  }
%\label{fig_3D_vectors}
%\end{figure}
%Lambertian model

%\section{Proposed Setup}
%why do we setup the kinects in such way, and what are the disadvantages and advantages of using multiple kinect for such setup.
%2x rgb 2x ir at different exposures
%can capture both stream at the same time, useful for video recording, and adding 

%how do we generate a high dynamic range image with the point cloud of different exposure.
%i.e., converting the rgbd (8 bits, 8 bits, 8 bits, 32 bits) to rgb (32 bits, 32 bits, 32 bits, 32 bits) and adding a vector that describe the lighting etc...
%show the math behind these algorithms (i.e., HDR composite, and ray tracing, the geometry problem and what we are solving)


\section{Wearable 3D HDR Digital Eye Glass}
We successfully created an array of Kinect cameras to capture differently exposed 3D data. In one example, using two Kinect cameras, where one is fitted with a neutral density filter, we obtained two simultaneously captured yet differently-exposed 3D sensing videos of the same subject matter.  We have tested the resulting 3DHDR camera on scenes with light shining directly into the camera. In particular, in the worst-case scenario where a tungsten light bulb that emits light in both visible spectrum and infrared spectrum was placed directly in the scene with no lamp shade, our 3DHDR camera produced excellent results, even though the individual 3D cameras failed. 

Our results suggest that by applying high dynamic range imaging technique to both the spatial and tonal domains, we can achieve a more robust tracking system and seeing aid for everyday use. To verify the feasibility of the system, we created a unique wearable 3DHDR eyeglasses prototype for everyday use. In Fig.~\ref{fig_wearable_glass}, a functional prototype which utilizes the PrimeSense 3D range sensing camera (the same technology that is used in the Kinect camera) and the EPSON Moverio head mounted display is shown. The prototype allows us to further explore the use of HDR imaging with 3D camera sensors in different scenarios. For example, the wearable setup allows users to experience an aug-mediated reality system which renders the high dynamic range 3D information from the POE (point of eye) of the wearer as shown in Fig.~\ref{fig_3D_HDR_results}).


%
%\begin{subfigure}[t]{.5\textwidth}
%\centering
%\includegraphics[width=2.3in]{ch4/diagrams/hdr_results/hdr2.jpg}
%\label{hdr_result}
%\caption{HDR Result}
%\end{subfigure}
%
%\begin{subfigure}{.5\textwidth}
%\centering
%\includegraphics[width=2.3in]{ch4/diagrams/hdr_results/no_hdr.jpg}
%\label{hdr_result1}
%\caption{3D Reconstruction}
%\end{subfigure}
%
%\begin{subfigure}{.5\textwidth}
%\centering
%\includegraphics[width=2.3in]{ch4/diagrams/hdr_results/hdr.jpg}
%\label{hdr_result2}
%\caption{Our proposed HDR result rendered in 3D}
%\end{subfigure}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[height=1.4in]{ch4/diagrams/hdr_results/low.jpg}
        \caption{Low Exposure}
        \label{hdr_low_expos}
    \end{subfigure}%
    ~ 
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[height=1.4in]{ch4/diagrams/hdr_results/high.jpg}
        \caption{High Exposure}
        \label{hdr_high_expos}
    \end{subfigure}
    ~
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[height=1.4in]{ch4/diagrams/hdr_results/hdr2.jpg}
        \caption{HDR Result}
        \label{hdr_result}
    \end{subfigure}
    \centering
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[height=1.8in]{ch4/diagrams/hdr_results/no_hdr.jpg}
        \caption{3D Reconstruction}
        \label{hdr_result1}
    \end{subfigure}
    ~
    \begin{subfigure}{0.55\textwidth}
	\centering	
	\includegraphics[height=1.8in]{ch4/diagrams/hdr_results/hdr.jpg}
	\caption{Final HDR result rendered in 3D}
	\label{hdr_result2}
    \end{subfigure}
    
    \caption{Example of 3DHDR rendering for wearable applications. Notice that the low exposure image captures the detail of the book cover but not the background. On the other hand, the high exposure image captures the background but the highlight details of surfaces such as the book cover and the mouse on the desk are saturated. By combining the images and the depth maps, we performed a 3D reconstruction of the scene from a virtual camera. By projecting the scene onto the wearer's POE, we can reconstruct a scene from the same perspective as how the user may see in real life~\cite{lo2013three}.}
\label{fig_3D_HDR_results}
\end{figure*}

\section{Discussion}
\label{sec_discussion}
This chapter proposed a method to reconstruct 3DHDR scenes using one or more Kinect cameras. However, there are several trade offs we considered in our proposed configuration. In particular, there are several limitations to be addressed in future work to produce high-quality 3DHDR videos for everyday use. 

\subsubsection{Interference among Kinect cameras}
The Kinect camera uses structured light patterns to reconstruct 3D data from the scene with the help of an IR laser projector. When multiple Kinect devices are used in the same space, interference can reduce the accuracy of the depth measurements. Several approaches have been proposed to mitigate this problem (e.g., \cite{maimone2012reducing}). One solution is to combine different 3D sensing technologies such as time-of-flight 3D sensors (e.g., SoftKinetic) to reduce the interference between sensors. 

\subsubsection{Programmable 3D cameras}
Unfortunately, the current Kinect camera does not provide fine grain control over the exposure settings and thus an ND filter is required for our current setup. In addition, the Kinect sensor does not provide hardware support in synchronizing the data streams, and thus we still suffer from a minor latency issue. 

\subsubsection{High Resolution and High Accuracy Depth Sensor}
One important limiting factor for the reconstruction stage is the spatial resolution and noise performance of the 3D sensors. In the future, a higher resolution depth map will significantly improve the overall accuracy and the quality of the output.

\subsubsection{Noise Modeling and Filtering}
Lastly, to improve the results we can employ various noise reduction algorithms for both the color image and depth map image. In particular, the depth map often shows missing data (e.g., holes and shadows) in the scene, as shown in Fig.~\ref{fig_3D_HDR_results}. These can be addressed by applying spatial and temporal filters \cite{matyunin2011temporal} on the depth map streams to improve our final results.

\section{Applications of 3DHDR and social implications}
One of the applications of our 3DHDR system is wearable computing.
An HDR camera system, in general, enables the user to capture a wider variety
of scenes than a regular camera which can only capture LDR scenes. That is,
an HDR camera system can see into dark and bright area of scenes `better' than
conventional cameras. Therefore, a wearable 3DHDR Digital Eye Glass wearable computing
device can act as a seeing aid by enhancing and protecting the viewers vision.
For example, through an HDR camera system, a user can look into the headlights
of a car in a dark alley and still see the license plate and the driver's face.
A user can also safely view welding
which would otherwise be harmful to the naked eye~\cite{mann2012hdrchitecture}.%
%However, if this is used in a wearable system, it is possible that, with prolonged use, the user's eyes will lose some of their natural dynamic range over time due to the lack of viewing high dynamic scenes with their naked eyes (need reference).

The 3DHDR system used in this fashion, as a seeing aid in a wearable computing platform, can be classified as a sousveillance system \cite{mann2004sousveillance,mann2006cyborglogging,mann2002sousveillance}, that is, instead of a camera system mounted on a building, which is surveillance, the camera is
human-centric and the world through this device is captured as a first person
view. Recently, there have been
a number of debates in the mass media over the appropriate use of sousveillance
devices, such as wearable camera systems. On one hand, sousveillance devices are viewed as
a deprivation of personal privacy that should be forbidden in any public area.
On the other hand, others view such devices as a revolutionary personal
assistance that can act as a visual aid and provide interactive
augmented/augmediated reality interactions \cite{hill2004reality} as the user sees it from a first-person view \cite{aimone2003eyetap}.

As a seeing aid, it is impractical for it to be forbidden, as those forbidding
it will assume liability for any side effects of non-usage (e.g., if a person
trips and falls because they were required to not wear their seeing aid).
Moreover, sousveillance may provide a necessary
balance-of-power between the authorities and individuals.

This work can further extended on 3D mapping and reconstruction technique such as KinectFusion, an real-time dense surface mapping and tracking technique which allows user to reconstruct a full 3D scene. By combining 3DHDR with KinectFusion, we can expect a high tonal range (HDR) 3D scene that can eventually be used in photo-realistic rendering to create surreal experience in virtual or augmented reality applications~\cite{agusanto2003photorealistic}. 

\section{Conclusion}
We successfully demonstrated a 3DHDR camera constructed from two
Kinect cameras, one of which was fitted with a neutral density filter.
The resulting differently exposed (one darker and one lighter) data streams
were merged in 3D to compensate for the parallax induced by the cameras
being at slightly different positions.

We were able to capture 3D information from high contrast scenes.
This suggests the possibility of making 3D cameras that can work in
outdoor settings or settings where the light is shining directly into
the camera. Thus, it is possible to build a 3D camera that can be used for
general-purpose sousveillance in real life, where the wearer does not
necessarily have complete control over the exact lighting conditions in the scene.

